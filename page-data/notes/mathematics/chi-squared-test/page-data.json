{"componentChunkName":"component---src-templates-post-tsx","path":"/notes/mathematics/chi-squared-test/","result":{"data":{"site":{"siteMetadata":{"title":"a cozy space"}},"markdownRemark":{"id":"89c509a8-ad27-5683-9cce-63e4e8448a5d","excerpt":"The chi-squared test is a statistical hypothesis test that is used to determine whether there is a significant difference between the observed and expected frequencies of one or more categories in a set of data.\nOnce more!\nGiven a hypothesis of what our data looks like (how it's distributed), the chi-squared test measures the difference between what we observe and what we would expect. The chi-squared test can only be used on categorical (i.e., nominal, discrete) data — so yes to the number of people in a certain gender, race/ethnicity, or political category, but no to heights, weights, lengths (unless you discretize your data first).\nUnlike other common hypothesis tests like the t-test, the chi-squared test is non-parametric, meaning that it does not rely on assumptions about the distribution of the data.\nInstead, it measures whether the data is likely to be drawn from a given distribution. The chi-squared test statistic has many talents—it can be used (i.e., interpreted) in numerous…","html":"<p>The chi-squared test is a statistical hypothesis test that is used to determine whether there is a significant difference between the observed and expected frequencies of one or more categories in a set of data.\nOnce more!\nGiven a hypothesis of what our data looks like (how it's <em>distributed</em>), the chi-squared test measures the difference between what we <em>observe</em> and what we would <em>expect</em>.</p>\n<p>The chi-squared test can only be used on <em>categorical</em> (<em>i.e.,</em> nominal, discrete) data — so yes to the number of people in a certain gender, race/ethnicity, or political category, but no to heights, weights, lengths (unless you discretize your data first).\nUnlike other common hypothesis tests like the t-test, the chi-squared test is <em>non-parametric</em>, meaning that it does not rely on assumptions about the distribution of the data.\nInstead, it measures whether the data is likely to be drawn from a given distribution.</p>\n<p>The chi-squared test statistic has many talents—it can be used (<em>i.e.,</em> interpreted) in numerous ways, such as:</p>\n<ul>\n<li>the chi-squared test of homogeneity,</li>\n<li>the chi-squared test of independence, and</li>\n<li>the chi-squared goodness-of-fit test,</li>\n</ul>\n<p>the mechanisms of which will be shortly discussed.</p>\n<h2>A Motivating Example</h2>\n<p>Let's say that we have gotten our curious hands on a dataset that consists of a random sample of two groups of people, $\\text{Group A}$ and $\\text{Group B}$.\nFor each observation in our dataset, we have information on whether that person has certain attributes — let's call them $\\text{Attribute 1}$ and $\\text{Attribute 2}$.\nWe may choose to summarize this data in a table like this:</p>\n<p>$$\n\\begin{array}{c|ccc}\n\\text{}         &#x26; \\text{Attribute 1} &#x26; \\text{Attribute 2} &#x26; \\text{Total} \\ \\hline \\\n\\text{Group A}  &#x26; N_{a1}             &#x26; N_{a2}             &#x26; N_a \\ \\\n\\text{Group B}  &#x26; N_{b1}             &#x26; N_{b2}             &#x26; N_b \\ \\\n\\text{Total}    &#x26; N_1                &#x26; N_2                &#x26; N\n\\end {array}\n$$</p>\n<p>where $N_{ij}$ is the number of people in group $i$ that have attribute $j$.\nA natural and solid thing for a wandering mind to consider at this point is: are the two groups different (with respect to the two attributes)?\nOr are they similar?\nWhat a loaded question!</p>\n<p>Let's suppose for a second that the two groups are similar.\nThen, we would expect that the probability for a person to have a certain attribute is the same, regardless of what group they are in.\nWe don't know what the <em>true</em> probabilities are (or we wouldn't need to ask these questions at all!), so let's make an educated guess given the data that we have.\nWe will use\n$$\np_1 = \\frac{N_1}{N}, \\text{ the probability for a person in our dataset to have attribute $1$} \\ \\hphantom{1} \\\np_2 = \\frac{N_2}{N}, \\text{ the probability for a person in our dataset to have attribute $2$}\n$$\nGiven these probabilities, we can calculate the number of people that we might expect to see with a given group/attribute combination.\nLet's give a more concrete example here:</p>\n<blockquote>\n<p>Suppose that the probability of a person having attribute $1$ is $p_1 = 0.3$.\nIf there are $N_a = 100$ people in group $A$, how many people would we expect to see in group $A$ that have attribute $1$?</p>\n</blockquote>\n<p>We would expect to see $N_ap_1 = 100(0.3) = 30$ people with that group/attribute combination.\nWe can generalize this to conclude that the <em>expected value</em> of the frequency of people in group $i$ with attribute $j$ is<br>\n$$\n\\mathbb{E}_{ij} = N_ip_j = N_i (\\frac{N_j}{N}) =  \\frac{N_iN_j}{N}\n$$</p>\n<p>Let's lay out what we have so far in two tables:\n$$\n\\begin{array}{c|ccc}\n\\textbf{Observed} &#x26; \\text{Attribute 1} &#x26; \\text{Attribute 2} \\ \\hline \\\n\\text{Group A}    &#x26; N_{a1}             &#x26; N_{a2}             \\ \\\n\\text{Group B}    &#x26; N_{b1}             &#x26; N_{b2}             \\ \\\n\\end {array}\n\\hspace{5em}\n\\begin{array}{c|ccc}\n\\textbf{Expected} &#x26; \\text{Attribute 1} &#x26; \\text{Attribute 2} \\ \\hline \\\n\\text{Group A}    &#x26; \\mathbb{E_{a1}}    &#x26; \\mathbb{E_{a2}}    \\ \\\n\\text{Group B}    &#x26; \\mathbb{E_{b1}}    &#x26; \\mathbb{E_{b2}}    \\ \\\n\\end {array}\n$$\nIf we were right in our assumption and the two groups are similar, we would expect the observed and expected values to be very close together!\nIn other words, the <em>difference</em> between the observed and expected values, $N_{ij}-\\mathbb{E}<em>{ij}$, should be very small.\nSo, let's add up these differences:\n$$\n(N</em>{a1}-\\mathbb{E}<em>{a1}) + (N</em>{a2}-\\mathbb{E}<em>{a2}) + (N</em>{b1}-\\mathbb{E}<em>{b1}) + (N</em>{b2}-\\mathbb{E}<em>{b2}) = \\sum</em>{i,j} (N_{ij}-\\mathbb{E}<em>{ij})\n$$\nBut wait! There's a problem here!\nLet's say that the difference for one combination is $-1000$ and the difference for another is $1000$.\nThese are both signifying that the observed and expected values are very different.\nBut when we add the two together, we get that the total difference is $0$!\nThis can't be right — what we need is for a big difference to have a big value.\nSo, let's solve this by squaring each difference:\n$$\n\\sum</em>{i,j} (N_{ij}-\\mathbb{E}_{ij})^2\n$$\nAh, much better.\nBut we look at it one more time, narrow our eyes, and realize that something else isn't quite right.\nLet's say that our dataset has many more people in group $A$ than in group $B$.\nThen, the expected value for one attribute in group $A$ would be bigger than the value for group $B$ — perhaps $1000$ for $A$ and $100$ for $B$.</p>\n<p>Now suppose that the difference in observed and expected values for this attribute is $200$ for $A$ and $50$ for $B$.\nThe difference for $A$ <em>looks</em> much worse at first glance, but we realize that it's not as bad as the difference for $B$: the difference for $A$ is only $20%$ of the expected value, but the difference for $B$ is a whopping $50%$!\nSo, we have to account for the size of the groups in our calculations.\nWe will <em>normalize</em> the data by dividing our squared difference by the expected value:\n$$\n\\sum_{i,j} \\frac{(N_{ij}-\\mathbb{E}<em>{ij})^2}{\\mathbb{E}</em>{ij}}\n$$\nAnd we sigh in relief.\nLet's summarize our process:</p>\n<ul>\n<li>We started with data that is split into categories (in this case, group/attribute combinations).</li>\n<li>We made a hypothesis about the distribution of our data (in this case, that the two groups have the same probabilities of having an attribute).</li>\n<li>We used these hypothesized probabilities to calculate the data that we would expect if our hypothesis were true.</li>\n<li>We calculated the magnitude of the difference between what we observe and what we expect, controlling for size.</li>\n</ul>\n<p>Congratulations, we have just recreated Pearson's chi-squared test statistic!</p>\n<h2>Definition of Pearson's Chi-Squared Test Statistic</h2>\n<p>In slightly different notation, we can define Pearson's chi-squared test statistic as follows:\n$$\n\\begin{align*}\n&#x26;\\textbf{Definition 1 (Pearson's chi-squared test statistic)}\n\\text{ Let $X_1, \\ldots, X_n$ be $n$ independent and identically-distributed (i.i.d.)} \\\n&#x26;\\text{observations, each of which belongs to one of $k$ categories. Pearson's chi-squared test statistic is defined as} \\\n\\end{align*} \\\n\\begin{align*}\n\\Chi^2 &#x26;= \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i} \\text{,} \\\n\\text{where } O_i &#x26;= \\text{the observed frequency of the $i$th category, and} \\\nE_i &#x26;= \\text{the expected frequency of the $i$th category.}\n\\end{align*}\n$$</p>\n<p>In 1900, Karl Pearson published a paper titled <em>On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling</em> (someone get this man an editor!), in which he proposed that as the number of observations goes to infinity (as $n \\to \\infty$), the test statistic $\\Chi^2_n$ converges in distribution to the $\\chi^2$ distribution.\n$$\n\\begin{align*}\n&#x26;\\textbf{Theorem 1 (Pearson's theorem)}\n\\text{ The random variable}\n\\end{align*} \\\n\\Chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i} \\longrightarrow^d \\chi^2_{k-1} \\text{,} \\\n\\text{\\textit{i.e.,} converges in distribution to the $\\chi^2$ distribution with $k-1$ degrees of freedom.}\n$$</p>\n<p>Why is this helpful in any way?\nThe basic premise is that now that we know it converges to a specific distribution, we can calculate <em>how likely</em> it is that we end up with a specific value for our test statistic $\\Chi^2$.</p>\n<h2>The Chi-Squared Distribution</h2>\n<p>Let's look a little more closely at the chi-squared distribution.\nAs we alluded to in the previous section, each chi-squared distribution is associated with a parameter called the <em>degree of freedom</em>.\nThe degree of freedom is the maximum number of independent variables in the system, <em>i.e.,</em> the number of parameters that are allowed to vary.<sup id=\"fnref-dof\"><a href=\"#fn-dof\" class=\"footnote-ref\">dof</a></sup>\nWe can see what chi-squared distributions for various degrees of freedom look like:</p>\n<p><sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>\n<img src=\"images/chi-squared-distribution-various-df.png\" alt=\"Chi-squared distribution for various degrees of freedom\"></p>\n<!-- [^dof-caption]: Source:  -->\n<!-- \nLet's suppose for a second that the two groups are similar. \nThen, we might expect them to have the same amount of people with attribute $1$ and attribute $2$. \nWait, that's not exactly right — what if there are 100 people in group $A$ and only 10 people in group $B$? \nGroup $A$ would very likely have more people with attribute $1$ or $2$ just by virtue of being a bigger group, not by virtue of the groups being dissimilar! \nThen, it might be better to compare *proportions*: the number of people with a certain attribute divided by the total number of people.\nThat way, we have *normalized* the data by the size of the group. \n\nSo, we would hypothesize that the proportion of people with a certain attribute is the same between groups. \nWhat would those hypothesized proportions be? \nUnfortunately, we don't have access to the *true* values — if we did, then we wouldn't need to ask these questions at all! \nPerhaps the best that we can do is use the data that we have. \nThe most representative data (of the population) that we have are the total number of people with each attribute, so let's assume that the true probability of a person having attribute $i$ is the number of people in our data with attribute $i$ divided by the number of total people in our data. \nSo, we define the following probabilities: \n$$\np_1 = \\frac{n_1}{n}, \\text{ the proportion of people with attribute $1$} \\\\ \\hphantom{1} \\\\\np_2 = \\frac{n_2}{n}, \\text{ the proportion of people with attribute $2$}\n$$\n\nAlright, so now we can proceed with the assumption that the probability that a person has attribute $i$ is $p_i$, regardless of which group they belong to. \nIf this is true, how many people would we expect to see with each group/attribute combination? \nLet's give a more concrete example here: \n> Suppose that the probability of a person having attribute $1$ is $p_1 = 0.3$. \n> If there are $n_a = 100$ people in group $A$, how many people would we expect to see in group $A$ that have attribute $1$? \nWe would expect to see $n_ap_1 = 100(0.3) = 30$ people with that group/attribute combination. \nWe can generalize this to conclude that the *expected value* of the frequency of people in group $i$ with attribute $j$ is  \n$$ \nn_ip_j = n_i (\\frac{n_in_j}{n}) =  \\frac{n_in_j}{n}.\n$$\n\n\n\nWhat would it mean for the two groups to be similar? \nWell, you might expect them to have the same amount of people with attribute $1$ and attribute $2$. \nWait, that's not exactly right — what if there are 100 people in group $A$ and only 10 people in group $B$? \nGroup $A$ would very likely have more people with attribute $1$ or $2$ just by virtue of being a bigger group, not by virtue of the groups being dissimilar! \nThen, it might be better to compare *proportions*: the number of people in a group with a certain attribute divided by the total number of people in the group. \nThat way, we have *normalized* the data by the size of the group. \n\nSo, let's define \n$$\np_1 = \\frac{n_1}{n}, \\text{ the proportion of people with attribute $1$} \\\\ \\hphantom{1} \\\\\np_2 = \\frac{n_2}{n}, \\text{ the proportion of people with attribute $2$}\n\nThen, we can now ask: is the proportion of people in group $A$ with attribute $1$ the same as the proportion of people in group $B$ with attribute $1$? \nHow about attribute $2$? \nHow would one approach an answer to this question? \n\nWhat would we *expect* to see if they were the same? \nWe would probably expect to see that they have the same proportions. \nSo, do we just compare the proportions that we've calculated? \n\nLet's clarify one thing. \nRemember that \n$$ -->\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-dof\">For example, let's say that you have a dataset of 10 counts. You know that the total count is 100. How many counts are you allowed to vary before you know the state of the dataset? You can vary 9 freely — but once you set those 9, you immediately know the 10th value (because the total must add up to 100). This system would have 9 degrees of freedom.<a href=\"#fnref-dof\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-1\">{-} Fig. 1. From Anthony Tanbakuchi; <a href=\"http://www.u.arizona.edu/~kuchi/Courses/MAT167/Files/LH_LEC.0640.HypTest.IndepHomog.pdf\">Tests of independence and homogeneity</a><a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","frontmatter":{"title":"Chi-squared test","subtitle":"Notes on the chi-squared test","date":"2022-08-07T00:00:00.000Z","lastupdated":"2022-08-07T00:00:00.000Z","topics":["notes","statistics","hypothesis testing"]}}},"pageContext":{"slug":"/notes/mathematics/chi-squared-test/","previous":{"fields":{"slug":"/notes/books/novels/1950-1999/vonnegut-kurt-slaughterhouse-five/"},"frontmatter":{"topics":["notes","kurt vonnegut","slaughterhouse-five","war","world war ii","time","science fiction","irony"],"title":"Vonnegut Jr., Kurt; Slaughterhouse-Five (1969)","layout":"booknote","published":true,"booknote":[{"author":"Kurt Vonnegut","title":"Slaughterhouse-Five"}]}},"next":{"fields":{"slug":"/notes/books/novels/1900-1949/kafka-franz-investigations-of-a-dog/"},"frontmatter":{"topics":["notes","investigations of a dog","kafka, franz","philosophy","absurdism"],"title":"Kafka, Franz; Investigations of a Dog (1922)","layout":"booknote","published":true,"booknote":[{"author":"Franz Kafka","title":"Investigations of a Dog"}]}}}},"staticQueryHashes":["712016698"]}